import streamlit as st
import torch
from transformers import AutoProcessor, LlavaForConditionalGeneration
from PIL import Image

# Page config
st.set_page_config(page_title="Crop Disease Detection", page_icon="üåø", layout="wide")

st.title("üåø Crop Disease Detection with AI-Generated Explanations")
st.markdown("Upload or capture a leaf image. The AI model analyzes it and generates disease info, symptoms, causes, and treatment advice directly.")

# Load model (cached)
@st.cache_resource
def load_model():
    model_id = "YuchengShi/LLaVA-v1.5-7B-Plant-Leaf-Diseases-Detection"
    model = LlavaForConditionalGeneration.from_pretrained(
        model_id,
        torch_dtype=torch.float16, 
        low_cpu_mem_usage=True, 
        device_map="auto"
    )
    processor = AutoProcessor.from_pretrained(model_id)
    return model, processor

try:
    with st.spinner("Loading AI Model... (this may take a moment)"):
        model, processor = load_model()
except Exception as e:
    st.error(f"Error loading model: {e}")
    st.stop()

def analyze_image(image):
    if image is None:
        return None
    
    # Prompt
    conversation = [
        {
            "role": "user",
            "content": [
                {"type": "text", "text": "Analyze this plant leaf image carefully. What crop is it? What disease (if any) does it show? Describe the visible symptoms in detail. What are the likely causes? How can it be treated or prevented? Provide confidence level if possible."},
                {"type": "image"},
            ],
        }
    ]
    
    prompt = processor.apply_chat_template(conversation, add_generation_prompt=True)
    inputs = processor(prompt, image, return_tensors="pt").to("cuda" if torch.cuda.is_available() else "cpu")
    
    # Generate
    with st.spinner("Analyzing Image..."):
        output = model.generate(**inputs, max_new_tokens=512, do_sample=False)
        generated_text = processor.decode(output[0], skip_special_tokens=True)
    
    # Extract response
    response = generated_text.split("assistant")[1].strip() if "assistant" in generated_text else generated_text
    return response

# Sidebar for inputs
with st.sidebar:
    st.header("Input Image")
    input_method = st.radio("Choose input method:", ["Upload Image", "Camera"])
    
    image = None
    if input_method == "Upload Image":
        uploaded_file = st.file_uploader("Choose an image...", type=["jpg", "jpeg", "png"])
        if uploaded_file is not None:
            image = Image.open(uploaded_file)
    elif input_method == "Camera":
        camera_input = st.camera_input("Take a picture")
        if camera_input is not None:
            image = Image.open(camera_input)

# Main area
col1, col2 = st.columns(2)

with col1:
    if image is not None:
        st.image(image, caption="Input Image", use_container_width=True)
        analyze_btn = st.button("Analyze Disease", type="primary")
    else:
        st.info("Please upload or capture an image to proceed.")
        analyze_btn = False

with col2:
    if analyze_btn and image is not None:
        analysis = analyze_image(image)
        if analysis:
            st.markdown("### üîç Model-Generated Analysis")
            st.markdown(analysis)
            st.caption("*(All information is generated by the LLaVA-v1.5-7B model)*")
